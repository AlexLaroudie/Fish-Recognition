{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database importations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4414/4414 [00:13<00:00, 334.93it/s]\n"
     ]
    }
   ],
   "source": [
    "images_path = [os.path.join('cropped_image', f) for f in os.listdir('cropped_image') if f.endswith('.png')]\n",
    "\n",
    "images_path = [i for i in tqdm(images_path) if len(np.array(Image.open(i)).shape) == 3] # remove grayscale images\n",
    "\n",
    "labels = [i.split('\\\\')[1].split('_')[:-1] for i in images_path]\n",
    "labels = ['_'.join(i) for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.DataFrame({'path': images_path, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4130 entries, 0 to 4129\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    4130 non-null   object\n",
      " 1   label   4130 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 64.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path     4130\n",
       "label     484\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cropped_image\\A73EGS-P_1.png</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cropped_image\\A73EGS-P_3.png</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cropped_image\\A73EGS-P_4.png</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cropped_image\\A73EGS-P_5.png</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cropped_image\\A73EGS-P_6.png</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>cropped_image\\zeus_faber_3.png</td>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>cropped_image\\zeus_faber_4.png</td>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>cropped_image\\zeus_faber_5.png</td>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>cropped_image\\zeus_faber_6.png</td>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>cropped_image\\zeus_faber_7.png</td>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4130 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                path       label  label_id\n",
       "0       cropped_image\\A73EGS-P_1.png    A73EGS-P         0\n",
       "1       cropped_image\\A73EGS-P_3.png    A73EGS-P         0\n",
       "2       cropped_image\\A73EGS-P_4.png    A73EGS-P         0\n",
       "3       cropped_image\\A73EGS-P_5.png    A73EGS-P         0\n",
       "4       cropped_image\\A73EGS-P_6.png    A73EGS-P         0\n",
       "...                              ...         ...       ...\n",
       "4125  cropped_image\\zeus_faber_3.png  zeus_faber       483\n",
       "4126  cropped_image\\zeus_faber_4.png  zeus_faber       483\n",
       "4127  cropped_image\\zeus_faber_5.png  zeus_faber       483\n",
       "4128  cropped_image\\zeus_faber_6.png  zeus_faber       483\n",
       "4129  cropped_image\\zeus_faber_7.png  zeus_faber       483\n",
       "\n",
       "[4130 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = df_total['label'].unique()\n",
    "num_unique_labels = [i for i in range(len(unique_labels))]\n",
    "\n",
    "df_labels = pd.DataFrame({'label': unique_labels, 'label_id': num_unique_labels})\n",
    "\n",
    "df_total = df_total.merge(df_labels, on='label', how='left')\n",
    "df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a pre-trained Vision Transformer (ViT) model, so we first pre-process the images in 4 steps\n",
    "\n",
    "- <b>Image resizing</b>: image resized to a fixed resolution of $224 \\times 224$ pixels. \n",
    "\n",
    "- <b>Normalization</b>: pixel values of the image are normalized across the RGB channels. This involves scaling the pixel values to have a mean of 0.5 and a standard deviation of 0.5 for each channel. \n",
    "\n",
    "- <b>Patch Extraction</b>: image is then divided into non-overlapping patches, each of size 16x16 pixels. This results in a sequence of patches that the model processes, treating each patch similarly to how tokens are treated in natural language processing tasks.\n",
    "\n",
    "- <b>Linear Embedding</b>: Each 16x16 patch is flattened into a 1-dimensional vector and then linearly transformed into an embedding of a specified dimension. This step translates the raw pixel data into a format suitable for input into the transformer's architecture.\n",
    "\n",
    "- <b>Position Embedding Addition</b>: Position embeddings are added to each patch to retain spatial information. These embeddings encode the position of each patch within the original image, allowing the model to understand the spatial relationships between patches.\n",
    "\n",
    "- <b>Class Token Addition</b>: A special classification token ([CLS]) is prepended to the sequence of patch embeddings. The final hidden state corresponding to this token is used as the aggregate representation of the entire image for classification purposes.\n",
    "\n",
    "All these steps are explained deeper on <a href = \"https://huggingface.co/google/vit-base-patch16-224\">the Hugging Face page</a> of the model and in the original publication <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. In the publication we could also find all informations on the model architecture etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate a Vision Transformer model specifically designed for image classification.\n",
    "\n",
    "It uses a pre-trained model identified by google that has been trained on the ImageNet-21k dataset\n",
    "\n",
    "This model is loaded with pre-trained weights and is used to adjust the classification head to match the number of classes in the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads an image from a path, pre-processes it using the defined processor (resizing, normalization, tensor conversion) and returns the pixel values in tensor form, ready for use by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, processor):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs['pixel_values']  # Extract only pixel values (tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a DataFrame containing image paths and their labels. It preprocesses each image using preprocess_image to obtain the pixel values and extracts the labels as tensors. It then creates a Dataset object containing the pixels and labels, ready for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df, path_col, labels_idx_col, processor):\n",
    "    preprocessed_images = []\n",
    "    labels = torch.tensor(df[labels_idx_col].values)\n",
    "\n",
    "    for i in tqdm(df[path_col].values, desc='Preprocessing images'):\n",
    "        pixel_values = preprocess_image(i, processor)  # Extract only pixel values (tensor)\n",
    "        preprocessed_images.append(pixel_values.squeeze())  # Squeeze the tensor if necessary to remove extra dimensions\n",
    "    \n",
    "    dataset_dict = {'pixel_values': preprocessed_images, 'labels': labels}\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions perform a prediction on a given image. It loads the image, preprocesses it with preprocess_image, sends the data to the same device as the model, and uses the model to obtain logits. The index of the highest score among the logits is returned as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(image_path, processor, model):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = preprocess_image(image_path, processor).to(model.device)\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    return predicted_class_idx\n",
    "\n",
    "def predict_from_image(image, processor, model):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    return predicted_class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4130/4130 [00:28<00:00, 144.10it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = preprocess_dataset(df_total, 'path', 'label_id', processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset_split = processed_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code configures training parameters, including batch sizes, number of epochs, learning rate and save and evaluate strategies. A Trainer object is then created to automate model training and evaluation on the appropriate device (CPU or GPU), using the datasets and processor provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOICE OF MODEL :\n",
    "\n",
    "The Vision Transformer (ViT) model is chosen for this project because of its ability to capture fine details and global relationships in images, which is essential for differentiating fish species. Pre-trained on large datasets such as ImageNet, it can be adjusted efficiently on specific datasets, even of small size. Its robustness to angle, lighting and background variations makes it a powerful choice for species classification in complex visual environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",          # Save after each epoch\n",
    "    per_device_train_batch_size=16, # Adjust based on GPU/CPU memory\n",
    "    per_device_eval_batch_size=16,  # Adjust based on GPU/CPU memory\n",
    "    num_train_epochs=3,             # Number of epochs\n",
    "    warmup_steps=500,               # Warmup steps for learning rate scheduler\n",
    "    learning_rate=5e-5,             # Learning rate\n",
    "    logging_dir='./logs',           # Logging directory\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset_split[\"train\"],\n",
    "    eval_dataset=processed_dataset_split[\"test\"],\n",
    "    tokenizer=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/621 [00:00<?, ?it/s]c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:252: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "  2%|â–         | 10/621 [00:14<13:02,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1977, 'grad_norm': 1.7483935356140137, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 20/621 [00:26<12:20,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2022, 'grad_norm': 1.6503273248672485, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–         | 30/621 [00:39<12:07,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1953, 'grad_norm': 1.9379818439483643, 'learning_rate': 3e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 40/621 [00:51<11:55,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1844, 'grad_norm': 1.7241199016571045, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 50/621 [01:04<11:43,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1934, 'grad_norm': 1.6593527793884277, 'learning_rate': 5e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 60/621 [01:17<12:57,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1747, 'grad_norm': 1.7218977212905884, 'learning_rate': 6e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–        | 70/621 [01:31<12:21,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1806, 'grad_norm': 1.6053146123886108, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 80/621 [01:44<12:07,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1832, 'grad_norm': 1.6769832372665405, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 90/621 [01:58<12:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1746, 'grad_norm': 1.719382405281067, 'learning_rate': 9e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 100/621 [02:12<12:10,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1743, 'grad_norm': 1.7115355730056763, 'learning_rate': 1e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 110/621 [02:25<11:42,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.18, 'grad_norm': 1.7326316833496094, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 120/621 [02:39<11:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1602, 'grad_norm': 2.5346083641052246, 'learning_rate': 1.2e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 130/621 [02:53<11:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1458, 'grad_norm': 1.8798775672912598, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 140/621 [03:07<10:58,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1525, 'grad_norm': 1.7022583484649658, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 150/621 [03:20<10:44,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.148, 'grad_norm': 1.8175896406173706, 'learning_rate': 1.5e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 160/621 [03:34<10:35,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1213, 'grad_norm': 1.8142632246017456, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 170/621 [03:47<09:25,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1382, 'grad_norm': 1.962619662284851, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 180/621 [03:59<09:05,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1176, 'grad_norm': 1.820120096206665, 'learning_rate': 1.8e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 190/621 [04:12<08:54,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1198, 'grad_norm': 2.07236909866333, 'learning_rate': 1.9e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 200/621 [04:24<08:48,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0903, 'grad_norm': 1.9042634963989258, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 207/621 [05:28<05:41,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0888447761535645, 'eval_runtime': 56.5691, 'eval_samples_per_second': 14.602, 'eval_steps_per_second': 0.919, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 210/621 [05:35<1:07:42,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0786, 'grad_norm': 1.9302380084991455, 'learning_rate': 2.1e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 220/621 [05:47<09:36,  1.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0332, 'grad_norm': 2.0529959201812744, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 230/621 [05:59<07:59,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0281, 'grad_norm': 2.090045690536499, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 240/621 [06:12<08:14,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0056, 'grad_norm': 2.1024014949798584, 'learning_rate': 2.4e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/621 [06:25<08:27,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0018, 'grad_norm': 1.8880449533462524, 'learning_rate': 2.5e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/621 [06:38<07:17,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.998, 'grad_norm': 2.0043134689331055, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 270/621 [06:50<07:03,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9829, 'grad_norm': 1.992096185684204, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 280/621 [07:02<06:52,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9811, 'grad_norm': 2.0707788467407227, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 290/621 [07:15<06:59,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9985, 'grad_norm': 2.255643367767334, 'learning_rate': 2.9e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 300/621 [07:28<07:11,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9585, 'grad_norm': 2.2047250270843506, 'learning_rate': 3e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 310/621 [07:41<06:14,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9629, 'grad_norm': 2.7350809574127197, 'learning_rate': 3.1e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/621 [07:53<06:39,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9664, 'grad_norm': 2.1389377117156982, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 330/621 [08:09<07:25,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9168, 'grad_norm': 2.0459482669830322, 'learning_rate': 3.3e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 340/621 [08:24<07:07,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.952, 'grad_norm': 2.1704373359680176, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 350/621 [08:39<06:57,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9026, 'grad_norm': 2.275416612625122, 'learning_rate': 3.5e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 360/621 [08:54<06:26,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.871, 'grad_norm': 2.0861480236053467, 'learning_rate': 3.6e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 370/621 [09:09<06:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9518, 'grad_norm': 2.106750249862671, 'learning_rate': 3.7e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 380/621 [09:24<05:52,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8844, 'grad_norm': 2.122260332107544, 'learning_rate': 3.8e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 390/621 [09:38<05:20,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8941, 'grad_norm': 1.987959861755371, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 400/621 [09:53<05:35,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8597, 'grad_norm': 2.0320940017700195, 'learning_rate': 4e-05, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 410/621 [10:08<05:15,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8599, 'grad_norm': 2.066953182220459, 'learning_rate': 4.1e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 414/621 [11:27<03:23,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.8559794425964355, 'eval_runtime': 74.3851, 'eval_samples_per_second': 11.104, 'eval_steps_per_second': 0.699, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 420/621 [11:39<18:02,  5.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7417, 'grad_norm': 2.1574792861938477, 'learning_rate': 4.2e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 430/621 [11:53<04:20,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.672, 'grad_norm': 2.179776430130005, 'learning_rate': 4.3e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 440/621 [12:06<04:13,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6693, 'grad_norm': 2.058309555053711, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 450/621 [12:20<03:53,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6403, 'grad_norm': 2.137808084487915, 'learning_rate': 4.5e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 460/621 [12:34<03:42,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.656, 'grad_norm': 2.1934173107147217, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 470/621 [12:46<03:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5869, 'grad_norm': 2.2831220626831055, 'learning_rate': 4.7e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 480/621 [12:58<02:50,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.645, 'grad_norm': 2.2567434310913086, 'learning_rate': 4.8e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 490/621 [13:11<02:38,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5948, 'grad_norm': 2.18607759475708, 'learning_rate': 4.9e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/621 [13:23<02:26,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6041, 'grad_norm': 2.1543853282928467, 'learning_rate': 5e-05, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 510/621 [13:35<02:15,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6027, 'grad_norm': 2.295408010482788, 'learning_rate': 4.586776859504133e-05, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 520/621 [13:47<02:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.559, 'grad_norm': 2.264561176300049, 'learning_rate': 4.1735537190082645e-05, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 530/621 [13:59<01:50,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5517, 'grad_norm': 2.201387405395508, 'learning_rate': 3.760330578512397e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 540/621 [14:12<01:42,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5406, 'grad_norm': 2.156916618347168, 'learning_rate': 3.347107438016529e-05, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 550/621 [14:24<01:28,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5478, 'grad_norm': 2.256890296936035, 'learning_rate': 2.9338842975206616e-05, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 560/621 [14:37<01:18,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5286, 'grad_norm': 2.048412799835205, 'learning_rate': 2.5206611570247934e-05, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 570/621 [14:51<01:11,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5643, 'grad_norm': 2.1092517375946045, 'learning_rate': 2.1074380165289255e-05, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 580/621 [15:04<00:52,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4578, 'grad_norm': 2.4177799224853516, 'learning_rate': 1.694214876033058e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 590/621 [15:17<00:40,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4192, 'grad_norm': 2.231384754180908, 'learning_rate': 1.2809917355371901e-05, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 600/621 [15:30<00:27,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4881, 'grad_norm': 2.283243417739868, 'learning_rate': 8.677685950413224e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 610/621 [15:43<00:15,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4326, 'grad_norm': 2.2273459434509277, 'learning_rate': 4.5454545454545455e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 621/621 [15:57<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4405, 'grad_norm': 2.721954107284546, 'learning_rate': 4.132231404958678e-07, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 621/621 [17:00<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.593548774719238, 'eval_runtime': 61.5946, 'eval_samples_per_second': 13.41, 'eval_steps_per_second': 0.844, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 621/621 [17:01<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1021.7472, 'train_samples_per_second': 9.701, 'train_steps_per_second': 0.608, 'train_loss': 5.890784626037794, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=621, training_loss=5.890784626037794, metrics={'train_runtime': 1021.7472, 'train_samples_per_second': 9.701, 'train_steps_per_second': 0.608, 'total_flos': 7.71418806058156e+17, 'train_loss': 5.890784626037794, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:59<00:00,  1.15s/it]\n",
      "c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35108958837772397\n",
      "Recall: 0.35108958837772397\n",
      "Precision: 0.30419012949101254\n",
      "F1: 0.28487774174108027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(processed_dataset_split[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(processed_dataset_split[\"test\"]['labels'], preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "recall = recall_score(processed_dataset_split[\"test\"]['labels'], preds, average='weighted')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "precision = precision_score(processed_dataset_split[\"test\"]['labels'], preds, average='weighted')\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "f1 = f1_score(processed_dataset_split[\"test\"]['labels'], preds, average='weighted')\n",
    "print(f'F1: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the model performs poorly, with an accuracy of 32.8%, a precision of 29.8%, a recall of 32.8% and an F1-score of 26.9%. These values indicate that the model struggles to differentiate classes correctly, with unreliable predictions and a limited balance between precision and recall. This suggests that the model requires improvement, either at the training level (hyperparameters, number of epochs), or at the data level (augmentation, class balancing, or enrichment), to better generalize and capture the essential features of the data.\n",
    "\n",
    "\n",
    "LIMITATIONS OF THE MODEL IN RELATION TO THE DATASET\n",
    "\n",
    "The results obtained are not satisfactory, mainly due to the large number of fish species to be classified and the low number of images available per species, with an average of only 5 images per class. This imbalance in the data makes it difficult for the model to learn representative features for each species, limiting its ability to generalize and make accurate predictions. A richer, more balanced dataset would be required to improve the model's performance.\n",
    "\n",
    "It is indeed far from its usual performances that are usually around 90% of accuracy according to these sources: <a href = \"https://dataloop.ai/library/model/google_vit-base-patch16-224/\">DataLoop</a>, <a href = \"https://paperswithcode.com/sota/image-classification-on-imagenet?p=deepvit-towards-deeper-vision-transformer\">Papers With Code</a> and in <a href = \"https://arxiv.org/pdf/2010.11929\">the original publication</a> on Table 2 and in the part C. ADDITIONAL RESULTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping and Description generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will scrap informations from the web to constitute a database of descriptions of the fishes we have in our dataset. \n",
    "\n",
    "We will use the information of <a href = \"https://www.fishbase.se/\">fishbase.se</a> which is a global biodiversity information system on all species currently known in the world. At present, FishBase covers >35,800 fish species compiled from >63,000 references.\n",
    "\n",
    "To scrap data we will use the <a href = \"https://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup</a> library which allows us to get informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import URLopener\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to understand how to scrap the website.\n",
    "\n",
    "An example of what the pages look like is <a href = \"https://www.fishbase.se/summary/zeus-faber.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_soup(url):\n",
    "    req = Request(url, headers={'User-Agent': 'Safari'})\n",
    "    webpage = urlopen(req, timeout=15).read()\n",
    "    to_ret = bs(webpage,\"html.parser\")\n",
    "    return to_ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         A73EGS-P\n",
       "1         A73EGS-P\n",
       "2         A73EGS-P\n",
       "3         A73EGS-P\n",
       "4         A73EGS-P\n",
       "           ...    \n",
       "4125    zeus_faber\n",
       "4126    zeus_faber\n",
       "4127    zeus_faber\n",
       "4128    zeus_faber\n",
       "4129    zeus_faber\n",
       "Name: label, Length: 4130, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus = df_total['label'].iloc[4012].split('_')[0] # let's use an example in our dataset\n",
    "species = df_total['label'].iloc[4012].split('_')[1]\n",
    "\n",
    "ret = gen_soup(f\"https://www.fishbase.se/summary/SpeciesSummary.php?ID=14550&genusname={genus}&speciesname={species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrap the \"Short description\" part of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dorsal spines (total): 7; Dorsal  soft rays (total): 20 - 24; Anal  spines : 3; Anal  soft rays'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# description\n",
    "\n",
    "to_take = str(ret).split('description')[1].split('smallSpace')[1].split('<span>')[1].split('</span>')[0].split('>')\n",
    "detailed = []\n",
    "for i in range(len(to_take)):\n",
    "    if 'href' in to_take[i]:\n",
    "        detailed.append(to_take[i].split('<a href=')[0])\n",
    "    else:\n",
    "        detailed.append(to_take[i])\n",
    "\n",
    "detailed = detailed[1:-1]\n",
    "result = [i.split('<')[0] if '<' in i else i for i in detailed]\n",
    "result = [i.split('(Ref')[0] if 'Ref' in i else i for i in result if i != '']\n",
    "result = [i for i in result if not i.isdigit()]\n",
    "\n",
    "result.remove(' ')\n",
    "\n",
    "description = ' '.join(result).replace('  ', ' ')\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the Biology part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adults occur near surface waters of lagoon and seaward reefs, in surge zones along sandy beaches , '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# biology\n",
    "\n",
    "bio_tab = str(ret).split('\t\t\t\t\tBiology\t\t\t\t\t')[1].split('smallSpace')[1].split('<span>')[1].split('</span>')[0].split('Ref')\n",
    "\n",
    "bio_tab = [bio_tab[i] for i in range(len(bio_tab)) if i % 2 == 0]\n",
    "bio_tab = [i.split('>')[-1] if '>' in i else i for i in bio_tab]\n",
    "bio_tab = [i.replace(').', '') if ').' in i else i for i in bio_tab]\n",
    "bio_tab = [i.replace(')', '') if ')' in i else i for i in bio_tab]\n",
    "bio_tab = [i.replace('(', '') if '(' in i else i for i in bio_tab]\n",
    "bio_tab = [i.replace('\\t', '') if '\\t' in i else i for i in bio_tab]\n",
    "bio_tab = [i.replace('\\n', '') if '\\n' in i else i for i in bio_tab]\n",
    "bio_tab = [i.replace('\\r', '') if '\\r' in i else i for i in bio_tab]\n",
    "\n",
    "biology = ' '.join(bio_tab).replace('  ', ' ')\n",
    "if '<' in biology:\n",
    "    biology = biology.split('<')[0]\n",
    "biology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do the same for the Distribution part and we can concatenate the 3 parts to have one \"clean\" sentence for the specie. \n",
    "The sentence is a good start but it is not truly user-friendly to read. To do that we will use an LLM to generate better description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short description: Dorsal spines (total): 7; Dorsal  soft rays (total): 20 - 24; Anal  spines : 3; Anal  soft rays\\nDistribution: Indo-Pacific:  Red Sea to the Line and MangarÃ©va islands, north to southern Japan, south to Lord Howe and Rapa.\\nBiology: Adults occur near surface waters of lagoon and seaward reefs, in surge zones along sandy beaches , '"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = str(ret).split('\t\t\t\t\tDistribution\t\t\t\t\t')[1].split('smallSpace')[1].split('<span>')[1].split('</span>')[0].split('\\t')[-1].split('<')[0]\n",
    "\n",
    "description = \"Short description: \" + description\n",
    "distribution = \"Distribution: \" + distribution\n",
    "biology = \"Biology: \" + biology\n",
    "\n",
    "full_desc = description + '\\n' + distribution + '\\n' + biology\n",
    "full_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to scrap automatically the data of the fishes of our dataset to create descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_fishbase(genus, species):\n",
    "    ret = gen_soup(f\"https://www.fishbase.se/summary/SpeciesSummary.php?ID=14550&genusname={genus}&speciesname={species}\")\n",
    "\n",
    "    try:\n",
    "        to_take = str(ret).split('\t\t\t\t\tShort description\t\t\t\t\t')[1].split('smallSpace')[1].split('<span>')[1].split('</span>')[0].split('>')\n",
    "        detailed = []\n",
    "        for i in range(len(to_take)):\n",
    "            if 'href' in to_take[i]:\n",
    "                detailed.append(to_take[i].split('<a href=')[0])\n",
    "            else:\n",
    "                detailed.append(to_take[i])\n",
    "\n",
    "        detailed = detailed[1:-1]\n",
    "        result = [i.split('<')[0] if '<' in i else i for i in detailed]\n",
    "        result = [i.split('(Ref')[0] if 'Ref' in i else i for i in result if i != '']\n",
    "        result = [i for i in result if not i.isdigit()]\n",
    "\n",
    "        result.remove(' ')\n",
    "\n",
    "        description = ' '.join(result).replace('  ', ' ')\n",
    "    except:\n",
    "        description = ''\n",
    "\n",
    "    # biology\n",
    "    try:\n",
    "        bio_tab = str(ret).split('\t\t\t\t\tBiology\t\t\t\t\t')[1].split('smallSpace')[1].split('<span>')[1].split('</span>')[0].split('Ref')\n",
    "\n",
    "        bio_tab = [bio_tab[i] for i in range(len(bio_tab)) if i % 2 == 0]\n",
    "        bio_tab = [i.split('>')[-1] if '>' in i else i for i in bio_tab]\n",
    "        bio_tab = [i.replace(').', '') if ').' in i else i for i in bio_tab]\n",
    "        bio_tab = [i.replace(')', '') if ')' in i else i for i in bio_tab]\n",
    "        bio_tab = [i.replace('(', '') if '(' in i else i for i in bio_tab]\n",
    "        bio_tab = [i.replace('\\t', '') if '\\t' in i else i for i in bio_tab]\n",
    "        bio_tab = [i.replace('\\n', '') if '\\n' in i else i for i in bio_tab]\n",
    "        bio_tab = [i.replace('\\r', '') if '\\r' in i else i for i in bio_tab]\n",
    "\n",
    "        biology = ' '.join(bio_tab).replace('  ', ' ')\n",
    "        if '<' in biology:\n",
    "            biology = biology.split('<')[0]\n",
    "    except:\n",
    "        biology = ''\n",
    "\n",
    "    try:\n",
    "        distribution = str(ret).split('\t\t\t\t\tDistribution\t\t\t\t\t')[1].split('smallSpace')[1].split('<span>')[1].split('</span>')[0].split('\\t')[-1].split('<')[0]\n",
    "    except:\n",
    "        distribution = ''\n",
    "    \n",
    "    if description != '':\n",
    "        description = \"Short description: \" + description\n",
    "    if distribution != '':\n",
    "        distribution = \"Distribution: \" + distribution\n",
    "    if biology != '':\n",
    "        biology = \"Biology: \" + biology\n",
    "\n",
    "    full_desc = description + '\\n' + distribution + '\\n' + biology\n",
    "\n",
    "    if full_desc == '\\n\\n':\n",
    "        return 'No information found'\n",
    "\n",
    "    return full_desc\n",
    "\n",
    "def scrap_to_apply(x):\n",
    "    if '_' in x:\n",
    "        genus = x.split('_')[0].replace(' ', '')\n",
    "        species = x.split('_')[1].replace(' ', '')\n",
    "\n",
    "        return scrap_fishbase(genus, species)\n",
    "    else:   \n",
    "        return 'No information found'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start scrapping the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>No information found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acanthaluteres_brownii</td>\n",
       "      <td>\\nDistribution: Eastern Indian Ocean: southern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acanthaluteres_spilomelanurus</td>\n",
       "      <td>Short description: Dorsal spines (total): 2; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>Short description: Dorsal spines (total): 2; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acanthistius_cinctus</td>\n",
       "      <td>\\nDistribution: Southwest Pacific.\\nBiology: O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>wetmorella_albofasciata</td>\n",
       "      <td>Short description: Dorsal spines (total): 9; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>wetmorella_nigropinnata</td>\n",
       "      <td>Short description: Dorsal spines (total): 9; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>xiphocheilus_typus</td>\n",
       "      <td>Short description: Dorsal spines (total): 12; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>zenarchopterus_dispar</td>\n",
       "      <td>Short description: Dorsal spines (total): 0; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>Short description: Dorsal spines (total): 9 - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             label  \\\n",
       "0                         A73EGS-P   \n",
       "1           acanthaluteres_brownii   \n",
       "2    acanthaluteres_spilomelanurus   \n",
       "3          acanthaluteres_vittiger   \n",
       "4             acanthistius_cinctus   \n",
       "..                             ...   \n",
       "479        wetmorella_albofasciata   \n",
       "480        wetmorella_nigropinnata   \n",
       "481             xiphocheilus_typus   \n",
       "482          zenarchopterus_dispar   \n",
       "483                     zeus_faber   \n",
       "\n",
       "                                           description  \n",
       "0                                 No information found  \n",
       "1    \\nDistribution: Eastern Indian Ocean: southern...  \n",
       "2    Short description: Dorsal spines (total): 2; D...  \n",
       "3    Short description: Dorsal spines (total): 2; D...  \n",
       "4    \\nDistribution: Southwest Pacific.\\nBiology: O...  \n",
       "..                                                 ...  \n",
       "479  Short description: Dorsal spines (total): 9; D...  \n",
       "480  Short description: Dorsal spines (total): 9; D...  \n",
       "481  Short description: Dorsal spines (total): 12; ...  \n",
       "482  Short description: Dorsal spines (total): 0; D...  \n",
       "483  Short description: Dorsal spines (total): 9 - ...  \n",
       "\n",
       "[484 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "descs = pd.DataFrame(df_total['label'].unique())\n",
    "descs.columns = ['label']\n",
    "descs['description'] = descs['label'].progress_apply(scrap_to_apply)\n",
    "descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let' save the informations found\n",
    "\n",
    "descs.to_csv('fishbase_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>No information found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acanthaluteres_brownii</td>\n",
       "      <td>\\nDistribution: Eastern Indian Ocean: southern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acanthaluteres_spilomelanurus</td>\n",
       "      <td>Short description: Dorsal spines (total): 2; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>Short description: Dorsal spines (total): 2; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acanthistius_cinctus</td>\n",
       "      <td>\\nDistribution: Southwest Pacific.\\nBiology: O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>wetmorella_albofasciata</td>\n",
       "      <td>Short description: Dorsal spines (total): 9; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>wetmorella_nigropinnata</td>\n",
       "      <td>Short description: Dorsal spines (total): 9; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>xiphocheilus_typus</td>\n",
       "      <td>Short description: Dorsal spines (total): 12; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>zenarchopterus_dispar</td>\n",
       "      <td>Short description: Dorsal spines (total): 0; D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>zeus_faber</td>\n",
       "      <td>Short description: Dorsal spines (total): 9 - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             label  \\\n",
       "0                         A73EGS-P   \n",
       "1           acanthaluteres_brownii   \n",
       "2    acanthaluteres_spilomelanurus   \n",
       "3          acanthaluteres_vittiger   \n",
       "4             acanthistius_cinctus   \n",
       "..                             ...   \n",
       "479        wetmorella_albofasciata   \n",
       "480        wetmorella_nigropinnata   \n",
       "481             xiphocheilus_typus   \n",
       "482          zenarchopterus_dispar   \n",
       "483                     zeus_faber   \n",
       "\n",
       "                                           description  \n",
       "0                                 No information found  \n",
       "1    \\nDistribution: Eastern Indian Ocean: southern...  \n",
       "2    Short description: Dorsal spines (total): 2; D...  \n",
       "3    Short description: Dorsal spines (total): 2; D...  \n",
       "4    \\nDistribution: Southwest Pacific.\\nBiology: O...  \n",
       "..                                                 ...  \n",
       "479  Short description: Dorsal spines (total): 9; D...  \n",
       "480  Short description: Dorsal spines (total): 9; D...  \n",
       "481  Short description: Dorsal spines (total): 12; ...  \n",
       "482  Short description: Dorsal spines (total): 0; D...  \n",
       "483  Short description: Dorsal spines (total): 9 - ...  \n",
       "\n",
       "[484 rows x 2 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate some well-written descriptions using LLM with the information we've found. \n",
    "\n",
    "We can write descriptions of 1000 tokens (token = part of a word or a word) maximum for readability of the users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.77s/it]\n"
     ]
    }
   ],
   "source": [
    "pipe_rewrite = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", device = device, max_new_tokens=1000 , pad_token_id = 128001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model description </h2>\n",
    "\n",
    "Llama-3.2-3B-Instruct is a 3-billion-parameter language model developed by Meta, optimized for instruction-following tasks. It employs an auto-regressive transformer architecture and has undergone supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align its outputs with human preferences for helpfulness and safety.\n",
    "\n",
    "As we use a little* model of only 3B from developped by Meta: Llama 3.2 3B, we need to well construct our prompt.\n",
    "\n",
    "Sources: <a href = \"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\">HuggingFace</a>, <a href = \"https://www.llama.com/\">llama.com</a>\n",
    "\n",
    "\n",
    "<h2>Prompting</h2>\n",
    "\n",
    "Prompting a small language model effectively is crucial because it ensures that the AI generates accurate, relevant, and context-specific responses. Well-designed prompts guide the model by clearly defining its role, task, and scope, minimizing ambiguity and maximizing the utility of the output. For the given prompt, the method employed includes: \n",
    "1. Role specification (\"You are an AI writing assistant\") to establish the AI's function \n",
    "\n",
    "2. Task definition (\"write a non-personal and complete fish description from the informations sent by the user\") to clarify expectations\n",
    "\n",
    "3. Explicit constraints (\"Do not return any text other than the rewritten description\") to prevent irrelevant or extraneous content. This structured approach ensures the prompt is precise, aligned with the desired outcome, and easy for the AI to interpret.\n",
    "\n",
    "So, we decide to prompt it the following instructions: \n",
    "\n",
    "- <u>System instruction:</u> You are an AI writing assistant. Your task is to write a non personal and complete fish description from the informations sent by the user. Do not return any text other than the rewritten description.\n",
    "\n",
    "- <u>Message form:</u> Write a one line non personal and complete fish description from the informations sent below. Do not add any new information or return any text other than the rewritten message\\nThe informations: ``given information``\n",
    "\n",
    "Sources: <a href = \"https://platform.openai.com/docs/guides/prompt-engineering\">OpenAI</a>\n",
    "\n",
    "<h2>Implementation</h2>\n",
    "\n",
    "We used a hugging face pipeline which is an easy-to-implement and effective way to use this type of model. \n",
    "<br><br><br><br><br><br><br>\n",
    "*we said the model is little because now there are models of more that 100B parameters like Mistral Large2, GPT-4o etc...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(df_key_words:pd.DataFrame, pipe_rewrite, output_csv:str):\n",
    "    key_words = df_key_words[\"description\"].values\n",
    "\n",
    "    system_prompt_rewrite = \"You are an AI writing assistant. Your task is to write user friendly and complete fish description from the informations sent by the user. Do not return any text other than the rewritten description.\"\n",
    "    user_prompt_rewrite = \"Write user friendly and complete fish description from the informations sent below. Do not add any new information or return any text other than the rewritten message\\nThe informations:\"\n",
    "\n",
    "    messages = [[{\"role\":\"system\", \"content\":system_prompt_rewrite},\n",
    "                  {\"role\":\"user\", \"content\":f\"{user_prompt_rewrite} {desc}\"}] for desc in key_words]\n",
    "    descriptions = []\n",
    "    for i in tqdm(range(len(messages))):\n",
    "        desc = pipe_rewrite(messages[i])\n",
    "        desc = desc[0]['generated_text'][2]['content']\n",
    "        descriptions.append(desc)\n",
    "\n",
    "        df_to_append = pd.DataFrame(data={\"label\":[df_key_words[\"label\"].values[i]],\"generated_description\": [desc]})\n",
    "\n",
    "        # Append the row to the CSV file\n",
    "        df_to_append.to_csv(output_csv, mode='a', header=not pd.io.common.file_exists(output_csv), index=False)\n",
    "\n",
    "\n",
    "    #descriptions = [desc[0]['generated_text'][2]['content'] for desc in descriptions]\n",
    "    df_to_ret = pd.DataFrame(data={\"label\": df_key_words[\"label\"].values, \"generated_description\": descriptions})\n",
    "\n",
    "    return df_to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 484/484 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "descs = pd.read_csv('fishbase_info.csv')\n",
    "df_descriptions = generate_description(descs[:31], pipe_rewrite, \"fish_descriptions_generated.csv\")\n",
    "df_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination of the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(img, processor, vision_model, full_df, df_labels):\n",
    "    pred = predict_from_image(img, processor, vision_model)\n",
    "    label = full_df[df_labels['label_id'] == pred]['label'].values[0]\n",
    "    desc = full_df[df_labels['label'] == label]['generated_description'].values[0]\n",
    "    return label, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: acanthaluteres_vittiger\n",
      "Description: This species of fish is identified by its distinctive dorsal spines, featuring a total of two. It boasts a notable characteristic of having 30-35 dorsal soft rays. Additionally, it possesses anal soft rays. Found in the Eastern Indian Ocean, this fish inhabits a vast area, including southern Australia, stretching from southern Western Australia to New South Wales and Tasmania.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_86728\\1110816710.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  label = full_df[df_labels['label_id'] == pred]['label'].values[0]\n",
      "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_86728\\1110816710.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  desc = full_df[df_labels['label'] == label]['generated_description'].values[0]\n"
     ]
    }
   ],
   "source": [
    "# example of utilisation\n",
    "df_descriptions = pd.read_csv('fish_descriptions_generated.csv')\n",
    "\n",
    "img = Image.open(df_total['path'].iloc[19])\n",
    "\n",
    "label, desc = full_pipeline(img, processor, model, df_descriptions, df_labels)\n",
    "\n",
    "print(f'Label: {label}')\n",
    "print(f'Description: {desc}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code, we can now predict the fish species and generate a description of the fish species from an image of the fish. \n",
    "\n",
    "However, currently the metrics shows the classification image model does not offer good results in comparison of its results on the benchmarks. We could improve its scores by using more data and training it for more epochs. The problematics of finding data could be solved by the users of the app: they take photos, some specialists can classify the photo of the fish taken and this data would be used to train the model. \n",
    "\n",
    "Moreover, we could add more species to our data to offer a better experience to the users and bring more help to the scientist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
